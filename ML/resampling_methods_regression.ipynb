{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General - Scikit-Learn Design\n",
    "All objects within scikit-learn share a unifrom common basic API consisting of three complementary interfaces: \n",
    "+ <b>estimator</b> <i>interface for building and fitting models</i>\n",
    "+ <b>predictor</b> <i>interface for making predictions</i>\n",
    "+ <b>transformer</b> <i>interface vor converting data.</i>\n",
    "\n",
    "### Estimators \n",
    "The estimator interface is at the core of the library. It defines instantiation mechanisms of objects and exposes a <em>fit()</em> method for learning a model from training data. Estimator initialization and actual learning are strictly separated, in a way that is similar to partial function application: an estimator is initialized from a set of named constant hyper-parameter values and can be considered as a function that maps these values to an actual learning algorithm. The constructor of an estimator does not see any actual data, nor does it perform any actual learning. All it does is attaching the given parameteres to the object. Default hyper-parameters values are also provided for all built-in estimators. These default values are set to be relevant in many common situation in order to make estimators as effective as possible out-of-box even for non-experts.\n",
    "Actual learning is performed by the <em>fit()</em> method. Its task is to run a learning algorithm and to determine model-specific parameters from the training data and set these attributes on the estimator object. \n",
    "\n",
    "### Predictors\n",
    "The predictor interface extends the notion of an estimator by adding a <em>predict()</em> method that takes an array $X\\_test$ and produces predictions for $X\\_test$, based on the  learned parameters of the estimator. Hence, in the case of supervised learning estimators, this method typically returns the predicted labels or values computed by the model. \n",
    "\n",
    "### Transformers\n",
    "Since it is common to modify or filter data before feeding it to a learning algorithm, some estimators in the library implement a transformer interface which defines a <em>transform()</em> method. It takes as input some new data, e.g., $X\\_test$ and yields as output a transformed version of $X\\_test$. Preprocessing, features selection, feature extraction and dimensionalty reduction algorithms are all provided as transformers within the library. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test Split\n",
    "\n",
    "The train and test split involves separating a dataset into two parts:\n",
    "\n",
    "* <b>Training Dataset.</b> The training dataset is used by the ML algorithm to train the model. \n",
    "* <b>Test Dataset.</b> The test dataset is held back and is used to evaluate the performance of the model.\n",
    "\n",
    "The rows assigned to each dataset are randomly selected. This is an attempt to ensure that the training and evaluation of a model is objective. If multiple algorithms are compared or multiple configurations of the same algorithm are compared, the same train and test split of the dataset should be used. This is to ensure that the comparison of performance is consistent. We can achieve this by seeding the random number generator the same way before splitting the data, or by holding the same split of the dataset for use by multiple algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import seed\n",
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "from numpy.random import choice\n",
    "import pandas as pd\n",
    "from math import floor\n",
    "\n",
    "# Split a dataset into a train and test set\n",
    "def train_test_split(dataset, split=0.60):\n",
    "    '''\n",
    "    Splitting dataset in train and test data.\n",
    "        \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : the dataset to be splitted\n",
    "    split : the ratio of data in the train data (default: 0.6)\n",
    "    Returns\n",
    "    -------\n",
    "    train_data, test_data: datasets containing the train data, respectively, test data \n",
    "    '''\n",
    "    n, p = dataset.shape\n",
    "    n_train = floor(n*split)\n",
    "    train_idx = choice(np.arange(n), replace=False, size=n_train)\n",
    "    test_idx = np.setdiff1d(np.arange(n), train_idx)\n",
    "    train_data = dataset.iloc[train_idx, :]\n",
    "    test_data = dataset.iloc[test_idx, :]\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90, 5)\n",
      "(60, 5)\n",
      "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
      "100                6.3               3.3                6.0               2.5   \n",
      "132                6.4               2.8                5.6               2.2   \n",
      "76                 6.8               2.8                4.8               1.4   \n",
      "64                 5.6               2.9                3.6               1.3   \n",
      "72                 6.3               2.5                4.9               1.5   \n",
      "\n",
      "     species  \n",
      "100      2.0  \n",
      "132      2.0  \n",
      "76       1.0  \n",
      "64       1.0  \n",
      "72       1.0  \n",
      "    sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
      "0                 5.1               3.5                1.4               0.2   \n",
      "5                 5.4               3.9                1.7               0.4   \n",
      "7                 5.0               3.4                1.5               0.2   \n",
      "9                 4.9               3.1                1.5               0.1   \n",
      "11                4.8               3.4                1.6               0.2   \n",
      "\n",
      "    species  \n",
      "0       0.0  \n",
      "5       0.0  \n",
      "7       0.0  \n",
      "9       0.0  \n",
      "11      0.0  \n"
     ]
    }
   ],
   "source": [
    "# test train/test split\n",
    "#example: iris data\n",
    "#http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html#sklearn.datasets.load_iris\n",
    "seed(42)\n",
    "iris = load_iris()\n",
    "X_data = iris.data\n",
    "target_data = iris.target.reshape(-1,1)\n",
    "iris_data = np.c_[X_data, target_data]\n",
    "def flatten(l): return flatten(l[0]) + (flatten(l[1:]) if len(l) > 1 else []) if type(l) is list else [l]\n",
    "colnames = [iris.feature_names, 'species']\n",
    "colnames = flatten(colnames)\n",
    "iris_data = pd.DataFrame(iris_data)\n",
    "iris_data.columns = colnames\n",
    "\n",
    "iris_train, iris_test = train_test_split(dataset = iris_data)\n",
    "\n",
    "print(iris_train.shape)\n",
    "print(iris_test.shape)\n",
    "\n",
    "print(iris_train.head(n=5))\n",
    "print(iris_test.head(n=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-fold Cross Validation Split\n",
    "\n",
    "A limitation of using the train and test split method is that we get a noisy estimate of algorithm performance.\n",
    "\n",
    "Therefore, the $\\textbf{k-fold cross validation}$ method has been introduced. It is also called just $\\textit{cross validation}$. Cross validation is a resampling method that provides a more accurate estimate of an algorithm performance.\n",
    "\n",
    "It does this by first splitting the data into $k$ groups. The algorithm is then trained and evaluated $k$ times and the performance summarized by taking the mean performance score. Each group of data is called a fold, hence the name $k$-fold cross-validation.\n",
    "\n",
    "It works by first training the algorithm on the $k-1$ groups of the data and evaluating it on the $k$-th hold-out group as the test set. This is repeated so that each of the $k$ groups is given an opportunity to be held out and used as the test set.\n",
    "\n",
    "As such, the value of $k$ should be divisible by the number of rows in your training dataset, to ensure each of the $k$ groups has the same number of rows.\n",
    "\n",
    "You should choose a value for $k$ that splits the data into groups with enough rows that each group is still representative of the original dataset. A good default to use is k=3 for a small dataset or k=10 for a larger dataset. A quick way to check if the fold sizes are representative is to calculate summary statistics such as mean and standard deviation and see how much the values differ from the same statistics on the whole dataset.\n",
    "\n",
    "We can reuse what we learned in the previous section in creating a train and test split here in implementing $k$-fold cross validation.\n",
    "\n",
    "Instead of two groups, we must return $k$-folds or $k$ groups of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##task 2 - implement k-fold cross-validation\n",
    "from random import seed\n",
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "from numpy.random import choice\n",
    "import pandas as pd\n",
    "from math import floor\n",
    "\n",
    "# Split a dataset into k folds\n",
    "def cross_validation_split(dataset, folds=3):\n",
    "    '''\n",
    "    Generating a cross validation split of a datasets. \n",
    "    The function splits the data into k-folds with k \n",
    "    being the number of folds.\n",
    "        \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : dataset which is splitted\n",
    "    folds: number of folds (default: 3)\n",
    "    Returns\n",
    "    -------\n",
    "    dataset_split: a list containing the separated folds\n",
    "    '''\n",
    "    n, p = dataset.shape\n",
    "    #create empty dictionary which stores all folds\n",
    "    k_folds = {fold: None for fold in map('fold{}'.format, range(1, folds+1))} \n",
    "    #compute split ratio for each cross-validation fold by ((k-1)/k * n) / n\n",
    "    split_ratio =  ((folds-1)/folds * n)/n\n",
    "    #for each fold call train_test_split function\n",
    "    for fold in k_folds:\n",
    "        train_data, test_data = train_test_split(dataset = dataset, split = split_ratio)\n",
    "        k_folds[fold] = {\"train\":train_data, \"test\":test_data}\n",
    "    return k_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112, 5)\n",
      "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
      "44                 5.1               3.8                1.9               0.4   \n",
      "85                 6.0               3.4                4.5               1.6   \n",
      "139                6.9               3.1                5.4               2.1   \n",
      "54                 6.5               2.8                4.6               1.5   \n",
      "68                 6.2               2.2                4.5               1.5   \n",
      "\n",
      "     species  \n",
      "44       0.0  \n",
      "85       1.0  \n",
      "139      2.0  \n",
      "54       1.0  \n",
      "68       1.0  \n",
      "(38, 5)\n",
      "    sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
      "3                 4.6               3.1                1.5               0.2   \n",
      "4                 5.0               3.6                1.4               0.2   \n",
      "7                 5.0               3.4                1.5               0.2   \n",
      "24                4.8               3.4                1.9               0.2   \n",
      "27                5.2               3.5                1.5               0.2   \n",
      "\n",
      "    species  \n",
      "3       0.0  \n",
      "4       0.0  \n",
      "7       0.0  \n",
      "24      0.0  \n",
      "27      0.0  \n",
      "(112, 5)\n",
      "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
      "16                 5.4               3.9                1.3               0.4   \n",
      "112                6.8               3.0                5.5               2.1   \n",
      "130                7.4               2.8                6.1               1.9   \n",
      "14                 5.8               4.0                1.2               0.2   \n",
      "17                 5.1               3.5                1.4               0.3   \n",
      "\n",
      "     species  \n",
      "16       0.0  \n",
      "112      2.0  \n",
      "130      2.0  \n",
      "14       0.0  \n",
      "17       0.0  \n",
      "(38, 5)\n",
      "    sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
      "6                 4.6               3.4                1.4               0.3   \n",
      "13                4.3               3.0                1.1               0.1   \n",
      "15                5.7               4.4                1.5               0.4   \n",
      "24                4.8               3.4                1.9               0.2   \n",
      "34                4.9               3.1                1.5               0.1   \n",
      "\n",
      "    species  \n",
      "6       0.0  \n",
      "13      0.0  \n",
      "15      0.0  \n",
      "24      0.0  \n",
      "34      0.0  \n"
     ]
    }
   ],
   "source": [
    "# test cross validation split\n",
    "iris_folds = cross_validation_split(iris_data, folds=4)\n",
    "\n",
    "print(iris_folds[\"fold1\"][\"train\"].shape)\n",
    "print(iris_folds[\"fold1\"][\"train\"].head())\n",
    "print(iris_folds[\"fold1\"][\"test\"].shape)\n",
    "print(iris_folds[\"fold1\"][\"test\"].head())\n",
    "\n",
    "\n",
    "print(iris_folds[\"fold2\"][\"train\"].shape)\n",
    "print(iris_folds[\"fold2\"][\"train\"].head())\n",
    "print(iris_folds[\"fold2\"][\"test\"].shape)\n",
    "print(iris_folds[\"fold2\"][\"test\"].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning curves by the number of training examples\n",
    "We now know how to use cross-validation which can be used to estimate of a model's generalization performance. Another way is to look at the <i>learning curves</i>. These are plots of the model's performance on the training set and the validation set <b>as a function of the training set size</b>. \n",
    "\n",
    "## TASKS\n",
    "* Overall Goal: train a model (scikit-learn model, e.g., LinearRegression) several times on different sized subsets of one training set. Plot the performance on the training set and the validation as a function of the training set size.\n",
    "\n",
    "Steps to go:\n",
    "\n",
    "* split the dataset into a training and test dataset\n",
    "* iterate the size of the training data\n",
    "* train the model being attached as a parameter and evaluate the training error, respectively, the error on the test data\n",
    "* plot the training error and test error (y-axis) by the size of the training set (x-axis) which has been used for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x17af5f674e0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCOgA0BIEdABoCQI6ALQEAR0AWuL/AfdaP3fIkq3aAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x17af2cc34a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# number of instances\n",
    "m = 100\n",
    "# dataset\n",
    "X = 6 * np.random.rand(m, 1) - 3\n",
    "y = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)\n",
    "# plot\n",
    "plt.figure()\n",
    "plt.scatter(np.sort(X), np.sort(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_learning_curve(model,X,y, test_sizes = [0.1, 0.2, 0.3, 0.4, 0.5]):\n",
    "    '''\n",
    "    Generating a plot of a learning curve\n",
    "        \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : model which is used for training the dataset\n",
    "    X: dataset\n",
    "    y: labels     \n",
    "    '''\n",
    "    no_iter = len(test_sizes)\n",
    "    performance_measure =  [None] * no_iter\n",
    "    i = 0\n",
    "    for size in test_sizes:\n",
    "        # Create machine learning model\n",
    "        ml_model = model\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= size, random_state=42)\n",
    "\n",
    "        # Train the model using the training sets\n",
    "        ml_model.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions using the testing set\n",
    "        y_pred = ml_model.predict(X_test)\n",
    "        performance_measure[i] = mean_squared_error(y_test, y_pred)\n",
    "        i += 1\n",
    "    plt.scatter(x=test_sizes, y=performance_measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nS5dJhYFvY1YAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x17af63b3a58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example for calling the function\n",
    "from sklearn.linear_model import LinearRegression\n",
    "ml_model = LinearRegression()\n",
    "plot_learning_curve(ml_model, X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularized linear models\n",
    "in the lecture as well as in the introduction to tensorflow, we have already seen a linear regression model. Here a short recap: the prediction of a linear regression model can be formulated as:\n",
    "\n",
    "$$\n",
    "\\hat{y} = h_\\theta (\\mathbf{x}) = \\theta_0 + \\theta_1 x_1 + \\ldots + \\theta_n x_n = \\theta^{T} \\cdot \\mathbf{x}\n",
    "$$\n",
    "\n",
    ", where $\\hat{y}$ is the predicted value, $n$ the number of features and $\\theta_j$ is the $j$-th model parameter.\n",
    "To measure how well (or poorly) the model fits the training data we can use the Mean Square Error. Hence, our goal is to find a value of $\\theta$ that minimizes the MSE.\n",
    "\n",
    "$$\n",
    "MSE (\\mathbf{X}, h_\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} (\\theta^{T} \\cdot x^i - y^i)^2\n",
    "$$\n",
    "\n",
    "One way to reduce overfitting is to regularize the model, i.e., to constrain it in its degrees of freedom. For a linear model, regularization means constraining the weights of the model. In this part of this tutorial, we will take a look at two common regularizations, called Ridge Regression and Lasso Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say the following dataset is given:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(42)\n",
    "m = 100\n",
    "X = 6 * np.random.rand(m, 1)\n",
    "y = 1 + 0.5 * X + .85 * np.random.randn(m, 1)\n",
    "plt.figure()\n",
    "plt.scatter(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASKS\n",
    "In the following the method plot_model shall be implemented. A model and various alphas for the regularizations are in the parameter list. Therefore, we iterate the alphas and apply them to the model. We use the sklearn framework described in the very beginning of this tutorial. We train the model (method to train a model in sklearn is the <em>fit()</em> method) on the dataset given above and calculate the predicted values (<em>predict()</em>). Lastly, we plot the models to see how well (or poor) the model fits the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_model(model_class, alphas, **model_kargs):\n",
    "    <fill_in>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression\n",
    "Ridge regression - also called Tikhonov regularization - performs a L2 regularization. That is to say it adds a penalty equivalent to the <b>square of the magnitude</b> of coefficients, i.e. the regularization term equal to $\\alpha \\sum_{i=1}^{n} \\theta_i^2$ is added to the cost function. \n",
    "\n",
    "$$\n",
    "J(\\theta) = MSE(\\theta) + \\alpha \\sum_{i=1}^{n} \\theta_i^2\n",
    "$$\n",
    "\n",
    "This forces the learning algorithm to keep the weights as small as possible. The hyperparameter $\\alpha$ controls how much the model is regularized. For $\\alpha=0$ Ridge Regression becomes Linear Regression. For a very large value for $\\alpha$, all weights end up very close to zero and the result is a flat line going through the data's mean. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "plot_model(Ridge, alphas=(0, 10, 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LASSO Regression \n",
    "Least Absolute Shrinkage and Selection Operator - for short LASSO - regression performs a L1 regularization. That is to say it adds a penalty equivalent to the <b>abolute value of the magnitude</b> of coefficients, i.e., the penalty term $\\alpha \\sum_{i=1}^n |\\theta_i|$ is added to the cost function.\n",
    "\n",
    "$$\n",
    "J(\\theta) = MSE(\\theta) + \\alpha \\sum_{i=1}^n |\\theta_i|\n",
    "$$\n",
    "\n",
    "One major characteristic of Lasso Regression is that it tends to eliminiate weights of the least important features and sets them to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "plot_model(Lasso, alphas=(0, 0.1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to play around with other models and their regularizations.\n",
    "\n",
    "# End of this tutorial"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
